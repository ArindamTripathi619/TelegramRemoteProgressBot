# Bot Monitor ğŸ¤–

**Universal process monitoring with LLM-powered analysis and Telegram notifications**

Monitor long-running processes on remote servers, get intelligent alerts when things go wrong. Perfect for ML training jobs, batch processing, deployments, or any long-running tasks where you need real-time failure notifications.

## Features

âœ… **Behavioral Sentinels**
- ğŸ“ˆ Log frequency spike detection
- ğŸ›‘ Log stream stall detection
- ğŸ§¬ Structural novelty detection (detecting new error types)
- ğŸ§  Historical duration learning (auto-estimates)

âœ… **Interactive Control**
- ğŸ•¹ï¸ Process status on demand (`/status`)
- â¸ï¸ Pause/Resume monitoring remotely
- ğŸ“‹ Live log snippets via Telegram (`/logs`)
- ğŸ Multi-stage progress tracking

âœ… **Optimized Efficiency**
- âš¡ Fuzzy caching (skeleton hashing to reduce LLM costs)
- âœ‚ï¸ Context trimming & timestamp stripping
- ğŸ“¦ Lightweight footprint (<20MB RAM)

## ğŸš€ Quick Start

### One-Command Setup

Run the interactive setup wizard - it will guide you through everything:

```bash
bot-monitor setup
```

**The wizard will:**
1. âœ… Collect and validate Telegram credentials (sends test message!)
2. âœ… Choose LLM provider and test API key
3. âœ… Configure monitors (files, PIDs, systemd services)
4. âœ… Validate file permissions and process access
5. âœ… Save complete working configuration
6. âœ… Offer to start monitoring immediately

**Example Session:**
```
ğŸ¤– Bot Monitor Setup Wizard
========================================

[1/5] Telegram Bot Configuration
----------------------------------------
To create a Telegram bot:
  1. Open Telegram and search for @BotFather
  2. Send: /newbot
  3. Follow instructions to create bot
  4. Copy the bot token

Enter Telegram Bot Token: 123456:ABC-DEF...
Enter your Telegram Chat ID: 987654321
âœ“ Validating Telegram setup...
âœ“ Success! Test message sent to Telegram!

[2/5] LLM Provider Configuration
----------------------------------------
Choose your LLM provider:
  1) OpenAI (Recommended, ~$0.15/1M tokens)
  2) Anthropic (High quality, ~$0.25/1M tokens)
  3) Groq (FREE tier, fast)
  4) Ollama (Local, private, FREE)

Choice [1-4]: 3
âœ“ Testing API key...
âœ“ API key validated! Model: llama-3.3-70b-versatile

[3/5] Monitor Configuration
----------------------------------------
Monitor log files? [Y/n]: y
  File path: /var/log/myapp.log
  âœ“ File found and readable
  Keywords to watch (comma-separated) [ERROR,FATAL,Exception]: 
  Monitor name [myapp.log]: 
  âœ“ File monitor configured: myapp.log

[4/5] Notification Settings
----------------------------------------
Max notifications per hour [10]: 15
âœ“ Rate limit: 15/hour

[5/5] Save Configuration
----------------------------------------
Configuration Summary:
  â€¢ Telegram: âœ“ Bot configured
  â€¢ LLM: âœ“ Groq (llama-3.3-70b-versatile)
  â€¢ Monitors: 1 configured
  â€¢ Rate limit: 15/hour

Save to ~/.config/bot-monitor/config.yaml? [Y/n]: y
âœ“ Configuration saved!

ğŸš€ Setup complete! What next?
  1) Start monitoring now
  2) Exit (start manually later)

Choice [1-2]: 1

Starting bot-monitor...
âœ“ File monitor: myapp.log
âœ“ Monitoring active. Press Ctrl+C to stop.
```

### Configuration

Edit `~/.config/bot-monitor/config.yaml`:

```yaml
telegram:
  bot_token: "${TELEGRAM_BOT_TOKEN}"
  chat_id: "${TELEGRAM_CHAT_ID}"

llm:
  provider: "openai"  # openai, anthropic, groq, ollama
  api_key: "${LLM_API_KEY}"
  model: "gpt-4o-mini"

notification:
  debounce_seconds: 300
  rate_limit_per_hour: 10
  severity_levels: [critical, warning, info]

monitors:
  # Monitor a log file
  - type: file
    name: "Training Logs"
    path: "/var/log/training.log"
    keywords: ["ERROR", "FATAL", "Exception"]
  
  # Monitor a process
  - type: pid
    name: "ML Training"
    pid: 12345
    check_interval: 30
  
  # Monitor systemd service
  - type: journal
    name: "Web Server"
    unit: "nginx.service"
```

### Interactive Commands

TeleWatch supports real-time interaction via your Telegram bot:

| Command | Action |
|---------|--------|
| `/status` | Get a detailed progress report with LLM summary |
| `/pause` | Temporarily suspend analysis and notifications |
| `/resume` | Re-activate monitoring |
| `/logs` | Get the last 15 lines of log output |

### Multi-Stage Tracking

Monitor complex pipelines by defining stages in your config:

```yaml
process:
  name: "Production Pipeline"
  stages:
    - name: "Data Sync"
      weight: 1
      start_pattern: "starting sync"
    - name: "ML Training"
      weight: 4
      start_pattern: "epoch 1"
    - name: "Deployment"
      weight: 1
      start_pattern: "deploying to prod"
```

TeleWatch will automatically calculate a weighted progress percentage and alert you on stage transitions.

## Use Cases

### ML Training Jobs
```yaml
monitors:
  - type: file
    name: "Training Logs"
    path: "/home/user/training/output.log"
    keywords: ["loss", "ERROR", "NaN", "diverged"]
  
  - type: pid
    name: "Training Process"
    pid: 98765
    check_interval: 60
```

### Web Deployments
```yaml
monitors:
  - type: journal
    name: "Application Server"
    unit: "myapp.service"
  
  - type: file
    name: "Nginx Error Log"
    path: "/var/log/nginx/error.log"
    keywords: ["error", "critical", "upstream"]
```

### Data Processing
```yaml
monitors:
  - type: file
    name: "ETL Pipeline"
    path: "/var/log/pipeline.log"
    keywords: ["FAILED", "TIMEOUT", "Exception"]
```

## LLM Provider Setup

### OpenAI (Recommended)
1. Get API key from [platform.openai.com](https://platform.openai.com/api-keys)
2. Set environment variable: `export LLM_API_KEY=sk-...`
3. Configure provider: `openai` with model `gpt-4o-mini`

### Groq (Fast & Free)
1. Get free API key from [console.groq.com](https://console.groq.com)
2. Set environment variable: `export LLM_API_KEY=gsk_...`
3. Configure provider: `groq` with model `llama-3.3-70b-versatile`

### Anthropic
1. Get API key from [console.anthropic.com](https://console.anthropic.com)
2. Set environment variable: `export LLM_API_KEY=sk-ant-...`
3. Configure provider: `anthropic` with model `claude-3-5-haiku-20241022`

### Ollama (Local)
1. Install [Ollama](https://ollama.ai)
2. Pull model: `ollama pull llama3.2`
3. Configure provider: `ollama` with model `llama3.2`
4. No API key needed!

## Environment Variables

Use environment variables for sensitive data:

```bash
export TELEGRAM_BOT_TOKEN="123456:ABC-DEF..."
export TELEGRAM_CHAT_ID="987654321"
export LLM_API_KEY="sk-..."
```

Then reference in config:
```yaml
telegram:
  bot_token: "${TELEGRAM_BOT_TOKEN}"
  chat_id: "${TELEGRAM_CHAT_ID}"
```

## Systemd Service

Run as a system service:

```bash
# Create service file
sudo nano /etc/systemd/system/bot-monitor.service
```

```ini
[Unit]
Description=Bot Monitor
After=network.target

[Service]
Type=simple
User=your-username
WorkingDirectory=/opt/bot-monitor
Environment="TELEGRAM_BOT_TOKEN=..."
Environment="LLM_API_KEY=..."
ExecStart=/path/to/venv/bin/bot-monitor start
Restart=on-failure

[Install]
WantedBy=multi-user.target
```

```bash
# Enable and start
sudo systemctl enable bot-monitor
sudo systemctl start bot-monitor
sudo systemctl status bot-monitor
```

## Troubleshooting

### "No configuration file found"
Run `bot-monitor setup` or create config at `~/.config/bot-monitor/config.yaml`

### "File not found" for log monitoring
Check file path exists and bot-monitor has read permissions

### "Process with PID X does not exist"
Update PID in config before starting

### Telegram messages not received
1. Check bot token is correct
2. Verify you've started a chat with the bot
3. Run `bot-monitor test-notification`
4. Check chat ID is your user ID, not group ID

### LLM API errors
1. Verify API key is set correctly
2. Check API quota/credits
3. Test with `bot-monitor test-notification`

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Configuration (YAML)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Monitor Manager                 â”‚
â””â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â”‚           â”‚              â”‚
  â–¼           â–¼              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚File  â”‚  â”‚ PID  â”‚      â”‚ Journal  â”‚
â”‚Watch â”‚  â”‚Track â”‚      â”‚ Reader   â”‚
â””â”€â”€â”¬â”€â”€â”€â”˜  â””â”€â”€â”¬â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
   â”‚         â”‚                â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Event Queue   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ LLM Analyzer  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   Telegram    â”‚
    â”‚   Notifier    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Contributing

Contributions welcome! Please feel free to submit pull requests.

## License

MIT License - See LICENSE file for details

## Support

For issues and questions:
- GitHub Issues: [Project Issues](https://github.com/DevCrewX/TelegramRemoteProgressBot/issues)
- Telegram: Create an issue for support

---

Built with â¤ï¸ for developers who need to monitor long-running processes without constant manual checking.
